coarse_module:
    backbone: 
        type  : "basic_encoder"
        kwargs:
            output_dim: 256
            norm_fn   : "instance"
    head:
        type: "FDRHead"

refine_module:
    backbone: 
        type  : "basic_encoder"
        kwargs:
            output_dim: 256
            norm_fn   : "instance"
    head:
        type: "FDRHead"

fourier_converter:
    beta_train: 0.06
    beta_test : 0.008
    platform  : "cv2" # platform to excute Fourier transform
                    # ["cv2", "torch"]

# size of target image
target_width : 768
target_height: 1088

# size of target inside doc
target_doc_w: 480
target_doc_h: 704

# grid size
grid_width : 9
grid_height: 9

train: False 

ckpt_path: None

debug: True

paths: 
    root_dir  : ${oc.env:PROJECT_ROOT}
    data_dir  : /content/WarpDoc
    log_dir   : ${paths.root_dir}/logs/
    output_dir: ${paths.root_dir}/checkpoints/

trainer:
    default_root_dir: ${paths.output_dir}

    min_epochs: 1 # prevents early stopping
    max_epochs: 100

    accelerator: gpu
    devices: 1
    num_nodes: 1

    # mixed precision for extra speed-up
    # precision: 16

    # perform a validation loop every N training epochs
    check_val_every_n_epoch: 1

    # set True to to ensure deterministic results
    # makes training slower but gives more reproducibility than just setting seeds
    deterministic: False
    benchmark: True

    accumulate_grad_batches: 1
    gradient_clip_val: 2.0
    precision: 32
    num_sanity_val_steps: 0
    limit_train_batches: 1.0
    limit_val_batches: 1.0
    sync_batchnorm: null
    strategy: auto 

log_frequency: 100

loss: 
    coarse: "L1"
    refine: "L1"
    mutual: 
        enable: True
        type  : "L1"
        m1    : 1.0
        m2    : 1.0
        weight: 0.5 # weight to balance

solver:
    name: "AdamW"
    lr: 0.0001
    momentum: 0.9
    decay_steps: [15,20]
    decay_gamma: 0.1
    layer_decay: null
    ZERO_WD_1D_PARAM: True
    warmup_epochs: 3
    weight_decay: 0.00005
    scheduler: "cosine"
    apply_linear_scaling: True

dataloader:
    batch: 2
    num_workers: 2
    pin_memory: True

dataset:
    brightness: 0.3
    constrast: 0.3
    sharpness: 0.3
    blur: 0.4
    geometry: 0.25

callbacks:
    model_checkpoint:  
        dirpath: ${paths.output_dir}
        filename: "epoch_{epoch:03d}"
        monitor: "step" #"val/mAP"
        mode: "max"
        save_last: True
        auto_insert_metric_name: False
        verbose: False # verbosity mode
        save_top_k: -1 # save k best models (determined by above metric)
        save_weights_only: False # if True, then only the modelâ€™s weights will be saved
        every_n_train_steps: null # number of training steps between checkpoints
        train_time_interval: null # checkpoints are monitored at the specified time interval
        every_n_epochs: 1 # number of epochs between checkpoints
        save_on_train_epoch_end: True # whether to run checkpointing at the end of the training epoch or the end of validation

    model_summary:
        max_depth: 1

    tqdm_progress_bar:
        refresh_rate: 1

    learning_rate_monitor:
        _target_: lightning.pytorch.callbacks.LearningRateMonitor

    timer:
        _target_: lightning.pytorch.callbacks.Timer
    
    ema:
      decay: 0.9999
      cpu_offload: False
      validate_original_weights: False
      every_n_steps: 1

logger:
    tensorboard:
        save_dir: "${paths.output_dir}/tensorboard/"
        version: 0