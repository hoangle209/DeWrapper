backbone: 
    type: "resnet50"

head:
    type: "FDRHead"

fourier_converter:
    beta_train: 0.06
    beta_test : 0.008
    platform  : "cv2" # platform to excute Fourier transform
                    # ["cv2", "torch"]

target_width : 2560
target_height: 3424
grid_width   : 9
grid_height  : 9

train: True 

paths: 
    root_dir  : ${oc.env:PROJECT_ROOT}
    data_dir  : link/to/dataset
    log_dir   : ${paths.root_dir}/logs/
    output_dir: ${paths.root_dir}/outs/

trainer:
    default_root_dir: ${paths.output_dir}

    min_epochs: 1 # prevents early stopping
    max_epochs: 100

    accelerator: gpu
    devices: 1
    num_nodes: 1

    # mixed precision for extra speed-up
    # precision: 16

    # perform a validation loop every N training epochs
    check_val_every_n_epoch: 1

    # set True to to ensure deterministic results
    # makes training slower but gives more reproducibility than just setting seeds
    deterministic: False
    benchmark: True

    accumulate_grad_batches: 1
    gradient_clip_val: 2.0
    precision: 32
    num_sanity_val_steps: 0
    limit_train_batches: 1.0
    limit_val_batches: 1.0
    sync_batchnorm: null
    strategy: null 

loss: 
    coarse: "L1"
    refine: "L1"
    mutual: 
        enable: True
        type  : "L1"
        m1    : 1.0
        m2    : 1.0
        weight: 0.5 # weight to balance

solver:
    name: "AdamW"
    lr: 0.0001
    momentum: 0.9
    decay_steps: [10,20]
    decay_gamma: 0.1
    layer_decay: null
    ZERO_WD_1D_PARAM: True
    warmup_epochs: 5
    weight_decay: 0.05
    scheduler: "cosine"
    apply_linear_scaling: True

dataloader:
    batch: 32
    num_workers: 2
    pin_memory: True

dataset:
    erasing: 0.4 # (float) probability of random erasing during classification training (0-1)
    blur: 0.4
    random_resize: False
    auto_augment: "randaugment"


